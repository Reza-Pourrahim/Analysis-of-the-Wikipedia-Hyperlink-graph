{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visit the Wikipedia hyperlinks graph!\n",
    "In this assignment we perform an analysis of the Wikipedia Hyperlink graph. In particular, given extra information about the categories to which an article belongs to, we are curious to rank the articles according to some criteria. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import defaultdict\n",
    "from heapq import *\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research questions\n",
    "\n",
    "\n",
    "### **[RQ1]** \n",
    "Build the graph <img src=\"https://latex.codecogs.com/gif.latex?G=(V,&space;E)\" title=\"G=(V, E)\" /> where *V* is the set of articles and *E* the hyperlinks among them, and provide its basic information:\n",
    " \n",
    "- If it is direct or not\n",
    "- The number of nodes\n",
    "- The number of edges \n",
    "- The average node degree. Is the graph dense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Build the graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty row\n"
     ]
    }
   ],
   "source": [
    "F = open('wiki-topcats-reduced.txt','r') \n",
    "rows=F.read().split('\\n') #split the rows\n",
    "grafo={}#initialize the graph\n",
    "reciver_nodes=set() #create a set of the target of the edges\n",
    "for row in rows:\n",
    "        link=row.split('\\t') \n",
    "        if link[0] not in grafo: #add the vertex if it doesn't exist\n",
    "            try:\n",
    "                grafo[link[0]]=set()\n",
    "                grafo[link[0]].add(link[1]) #add the edge\n",
    "                reciver_nodes.add(link[1])\n",
    "            except: print('empty row')\n",
    "        else:\n",
    "            grafo[link[0]].add(link[1])\n",
    "            reciver_nodes.add(link[1])\n",
    "F.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add to our graph nodes that only recives edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyreciver=(reciver_nodes-set(grafo.keys()))\n",
    "for node in onlyreciver:\n",
    "    grafo[node]=set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Find out if it's directed or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for neighbors in grafo['52']:\n",
    "    print('52' in grafo[neighbors])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that it's directed, since we have unspecular edges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get the number of nodes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "461194"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes=len(grafo)\n",
    "nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get the number of edges!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2645247"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges=0\n",
    "for node in grafo:\n",
    "    try: edges+=(len(grafo[node]))\n",
    "    except: pass\n",
    "edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get the average node degree. Is the graph dense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In graph theory, the degree of a vertex of a graph is the number of edges incident to the vertex. The degree of a vertex $v$ is denoted $\\deg(v)$.\n",
    "\n",
    "The average degree is denoted as $\\frac{E}{N}$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.735649206190887"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_degree= edges/nodes\n",
    "avg_degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the average node degree is slightly great than six.\n",
    "In mathematics, a dense graph is a graph in which the number of edges is close to the maximal number of edges.\n",
    "Looking at our numbers, we can immediatly say that the graph is not dense, but very sparse.\n",
    "\n",
    "To have a matematical confirm of this we can compute the density as $D={\\frac{|E|}{|N|\\,(|N|-1)}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2436548703451455e-05"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "density = edges/(nodes*(nodes-1))\n",
    "density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our hypothesis is verified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For completeness we create a dictionary that maps every page with it's name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = open('wiki-topcats-page-names.txt','r')\n",
    "articles={}\n",
    "for line in F.readlines():\n",
    "    num=line.split()[0]\n",
    "    tit=line.split()[1:]\n",
    "    title=' '.join(tit)\n",
    "    articles[num]=title\n",
    "F.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **[RQ2]** \n",
    "Given a category <img src=\"https://latex.codecogs.com/gif.latex?C_0&space;=&space;\\{article_1,&space;article_2,&space;\\dots&space;\\}\" title=\"C_0 = \\{article_1, article_2, \\dots \\}\" /> as input we want to rank all of the nodes in *V* according to the following criteria:\n",
    "\t\n",
    "* Obtain a *block-ranking*, where the blocks are represented by the categories. In particular, we want:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?block_{RANKING}&space;=\\begin{bmatrix}&space;C_0&space;\\\\&space;C_1&space;\\\\&space;\\dots&space;\\\\&space;C_c\\\\&space;\\end{bmatrix}\" title=\"block_{RANKING} =\\begin{bmatrix} C_0 \\\\ C_1 \\\\ \\dots \\\\ C_c\\\\ \\end{bmatrix}\" />\n",
    "\t\n",
    "Each category $C_i$ corresponds to a list of nodes. \n",
    "\n",
    "The first category of the rank, $C_0$, always corresponds to the input category. The order of the remaining categories is given by:\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?$$distance(C_0,&space;C_i)&space;=&space;median(ShortestPath(C_0,&space;C_i))$$\" title=\"distance(C_0, C_i) = median(ShortestPath(C_0, C_i))\" />\n",
    "\n",
    "The lower is the distance from $C_0$, the higher is the $C_i$ position in the rank. $ShortestPath(C_0, C_i)$ is the set of all the possible shortest paths between the nodes of $C_0$  and $C_i$. Moreover, the length of a path is given by the sum of the weights of the edges it is composed by.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creating the categories dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a dictionary that associate evry category with his articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = open('wiki-topcats-categories.txt','r')\n",
    "categorie={}\n",
    "for line in F.readlines():\n",
    "    riga=line.split(' ')\n",
    "    categoria=(riga[0].replace('Category:','').replace(';',''))\n",
    "    articles=(riga[1:-1])\n",
    "    articles.append(riga[-1].replace('\\n',''))\n",
    "    categorie[categoria]= articles\n",
    "F.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must take into account all the categories that has a number of articles greater than 3500, so we clean up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories={}\n",
    "for cat in categorie:\n",
    "    if len(categorie[cat])>=3500:\n",
    "        categories[cat]=categorie[cat]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove every article that is not in our graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in categories:\n",
    "    toremove=[]\n",
    "    for art in categories[cat]:\n",
    "        if art not in grafo:\n",
    "            toremove.append(art)\n",
    "    for art in toremove:\n",
    "        categories[cat].remove(art)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store our final categories dictionary in an external pickle file so we can reload it in need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('categories.pickle', 'wb') as handle:\n",
    "    pickle.dump(categories, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('categories.pickle', 'rb') as handle:\n",
    "    categories = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The block ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/DijkstraDemo.gif/220px-DijkstraDemo.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to solve the problem with dijkstra algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance between two nodes:\n",
    "\n",
    "def dijkstra(graph, f, t): #graph, from, to\n",
    "    #sorting the graph\n",
    "    g = defaultdict(list, grafo)\n",
    "    q= [(0,f)] #intializing the queque\n",
    "    seen=set() #initializing the set of seen nodes\n",
    "    distances = {f: 0} #initializing the dict of the distances\n",
    "    while q: #we run the algo until the queue is not empty\n",
    "        (cost,v1) = heappop(q)\n",
    "        if v1 not in seen:\n",
    "            seen.add(v1)\n",
    "            if v1 == t: return (cost) #if we reach the target, we stop\n",
    "            if g.get(v1) is not None:  #if the vertex that we are examining has children, we continue our search\n",
    "                for v2 in g.get(v1):\n",
    "                    if v2 in seen: continue\n",
    "                    prev = distances.get(v2, None)\n",
    "                    new = cost + 1 #increasing the distance\n",
    "                    if prev is None or new < prev:\n",
    "                        distances[v2] = new\n",
    "                        heappush(q, (new, v2))\n",
    "\n",
    "    return float(\"inf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distance between categories\n",
    "\n",
    "def catdistance(graph, catdict, cat1, cat2): #graph, dictionary of categories, and two categories\n",
    "    cat1nodes= catdict[cat1]\n",
    "    cat2nodes= catdict[cat2]\n",
    "    distances=[]\n",
    "    for node in cat1nodes:\n",
    "        for node2 in cat2nodes:\n",
    "            distances.append(dijkstra(graph=graph, f=node, t=node2))\n",
    "    return np.median(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catdistance(cat1='Japanese_rock_music_groups', cat2='Visual_kei_bands', catdict=categories, graph=grafo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works but is reeeeally slow. We want something more efficient. We can try a different approach using BFS algorithm, since the edges are not weighted.\n",
    "\n",
    "BFS should be faster since his running time is something like $\t\\mathcal{O}{\\bigl (}N+E{\\bigr )} $, while dijkstra's is ${\\displaystyle \\mathcal{O}{\\bigl (}E+N^{2}{\\bigr )}=\\mathcal{O}{\\bigl (}N^{2}{\\bigr )}}$ .\n",
    "\n",
    "Morover, we can reduce drastically our runningtime creating a dictionary that associate every node of the graph with the distances from every node of the input category. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BFS](https://sadmanamin.com/wp-content/uploads/2017/07/videotogif_2017.07.01_20.55.06.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function that takes a graph and a node as input and return a dictionary of his distance from every other node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bfs(graph, node):\n",
    "    shortestpath={} #the dict \n",
    "    visited=set() #set of vertex that we have already seen\n",
    "    queue=[node] #a queue of node that we have to examine\n",
    "    dist=0 #at first we are at distance 0 from the node, we increase this count as we step away\n",
    "    while queue: #if the queue is not empty\n",
    "        vicini = [] #initialize an empty list of neighbours of the nodes\n",
    "        for vertex in queue: #for each vertex of a given distance from the node\n",
    "            if vertex not in visited: \n",
    "                visited.add(vertex) #we add to the set of the visited nodes\n",
    "                neighbours = graph[vertex] #we add his neihgbours to the list\n",
    "                vicini.extend(neighbours - visited) #we don't want to have duplicates\n",
    "                shortestpath[vertex] = dist #we add the node to the dictionary\n",
    "        queue=vicini #we reset the queue as the list of neighbours, that we are going to analize next\n",
    "        dist += 1 #we increase the distance, and we are ready to repeat the cycle\n",
    "    return shortestpath #we finally have our dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want a function that collects the bfs that starts from every node of the input category.\n",
    "\n",
    "We also want to store this list in an external file since it's a long operation and we don't want to repeat it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solutiondata(graph,inp_cat, catdict): #input: a graph, an input category and a dictionary of categories\n",
    "    inpnodes=catdict[inp_cat] #nodes of the input category\n",
    "    bfssol=[] #the list of dictionary of distances from every point of the graph\n",
    "    for node in tqdm(inpnodes):\n",
    "        bfssol.append(bfs(grafo, node)) \n",
    "    with open('bfssol.pickle', 'wb') as handle: #store the list in an external file\n",
    "        pickle.dump(bfssol, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving the task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year_of_birth_unknown\n"
     ]
    }
   ],
   "source": [
    "inputcat=input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "solutiondata(catdict=categories, graph=grafo, inp_cat=inputcat) #we create our list of dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bfssol.pickle', 'rb') as handle:\n",
    "    bfssol = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to create a dict (in some sense an 'inverted index') that has as keys every node of the graph and as values every shortespath from the nodes of the input category (now we have as keys the node of the input category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldict = {}\n",
    "\n",
    "for diz in (bfssol):\n",
    "    for vert, value in diz.items():\n",
    "        if vert not in finaldict:\n",
    "            finaldict[vert] = [value] \n",
    "        else:\n",
    "            finaldict[vert].append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('finaldict.pickle', 'wb') as handle:\n",
    "    pickle.dump(finaldict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('finaldict.pickle', 'rb') as handle:\n",
    "    finaldict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally build our blockranking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'Year_of_birth_unknown')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blockranking=[(0, inputcat)]\n",
    "blockranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [01:57<00:00,  1.11it/s]\n"
     ]
    }
   ],
   "source": [
    "lung= len(categories[inputcat]) #we will need this to check how many impossible paths we have for each pair \n",
    "                                #between the input cat and the others\n",
    "for cat in tqdm(categories): #for every category except the input one\n",
    "    if cat!=inputcat:\n",
    "        paths=[] #every path between category nodes and input cat nodes\n",
    "        catnodes=categories[cat]\n",
    "        for node in catnodes:\n",
    "            if node in finaldict: #if at least a path exist\n",
    "                paths+=(finaldict[node]) #we add every path\n",
    "                imppaths= lung-len(finaldict[node])\n",
    "                paths+=([float('inf')]*imppaths) #we add the impossible paths marking them with \"infinite\" distance\n",
    "            else:\n",
    "                paths+=([float('inf')]*lung) #if we have no path, we add only impossible ones.\n",
    "        median=np.median(paths) #the median of the path\n",
    "        if median==float('inf'): #we want something that differenciate the distances that have as a median an infinite,\n",
    "                                 #so we set the median as 10000 plus the number of impossible paths\n",
    "            median=10000+paths.count(float('inf'))\n",
    "        blockranking.append(( median, cat)) #we append the category to the blockranking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "blockranking=(sorted(blockranking))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('blockranking.pickle', 'wb') as handle:\n",
    "    pickle.dump(blockranking, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('blockranking.pickle', 'rb') as handle:\n",
    "    blockranking = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final blockranking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'Year_of_birth_unknown'),\n",
       " (6.0, 'American_film_actors'),\n",
       " (6.0, 'American_films'),\n",
       " (6.0, 'American_television_actors'),\n",
       " (6.0, 'British_films'),\n",
       " (6.0, 'English-language_films'),\n",
       " (6.0, 'English_television_actors'),\n",
       " (7.0, 'American_Jews'),\n",
       " (7.0, 'Article_Feedback_Pilot'),\n",
       " (7.0, 'Black-and-white_films'),\n",
       " (7.0, 'Indian_films'),\n",
       " (7.0, 'Members_of_the_United_Kingdom_Parliament_for_English_constituencies'),\n",
       " (7.0, 'People_from_New_York_City'),\n",
       " (8.0, 'Debut_albums'),\n",
       " (8.0, 'English-language_albums'),\n",
       " (8.0, 'Fellows_of_the_Royal_Society'),\n",
       " (8.0, 'Rivers_of_Romania'),\n",
       " (9.0, 'Place_of_birth_missing_(living_people)'),\n",
       " (9.0, 'Windows_games'),\n",
       " (10.0, 'American_military_personnel_of_World_War_II'),\n",
       " (10.0, 'Living_people'),\n",
       " (10.0, 'The_Football_League_players'),\n",
       " (11.0, 'English_cricketers'),\n",
       " (11.0, 'Harvard_University_alumni'),\n",
       " (13.0, 'English_footballers'),\n",
       " (4971590, 'Association_football_goalkeepers'),\n",
       " (5502511, 'Association_football_defenders'),\n",
       " (5657119, 'Major_League_Baseball_pitchers'),\n",
       " (5722046, 'Association_football_forwards'),\n",
       " (6407609, 'Year_of_death_missing'),\n",
       " (6552150, 'Year_of_birth_missing'),\n",
       " (7065574, 'Association_football_midfielders'),\n",
       " (8262170, 'Asteroids_named_for_people'),\n",
       " (20633398, 'Main_Belt_asteroids'),\n",
       " (29670880, 'Year_of_birth_missing_(living_people)')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blockranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we obtained the $block_{RANKING}$ vector, we want to sort the nodes in each category. That's how we are going to do:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Suppose the categories order, given from the previous point, is $C_0, C_1, C_2$\n",
    "\n",
    "\n",
    "__[STEP1]__ Compute subgraph induced by $C_0$. For each node compute the sum of the weigths of the in-edges.\n",
    "\n",
    " <img src=\"https://latex.codecogs.com/gif.latex?score_{article_i}&space;=&space;\\sum_{i&space;\\in&space;in-edges}&space;w_i\" title=\"score_{article_i} = \\sum_{j \\in in-edges(article_i)} w_j\" />\n",
    "\n",
    "__[STEP2]__ Extend the graph to the nodes that belong to $C_1$. Thus, for each article in $C_1$ compute the score as before. __Note__ that the in-edges coming from the previous category, $C_0$, have as weights the score of the node that sends the edge.\n",
    "\n",
    "\n",
    "__[STEP3]__ Repeat Step2 up to the last category of the ranking. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If an article is in more than one category, we want him only in the nearest category to the input category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 35/35 [02:26<00:00,  4.20s/it]\n"
     ]
    }
   ],
   "source": [
    "visti= set() #a set of already seen vectors\n",
    "for el in tqdm(blockranking): #every category in order\n",
    "    cat=el[1] \n",
    "    nodi=[]\n",
    "    nodi+=categories[cat]\n",
    "    for node in nodi: #for every node of the category\n",
    "        if node not in visti: #if is the first time that we meet him\n",
    "            visti.add(node) #we add it to the set\n",
    "        else: \n",
    "            categories[cat].remove(node) #we remove them from the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('categoriesclean.pickle', 'wb') as handle:\n",
    "    pickle.dump(categories, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('categoriesclean.pickle', 'rb') as handle:\n",
    "    categories = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we introduce the library networkx, in order to have the possibility to do some operation with the helpful directed graphs class\n",
    "G=nx.DiGraph(incoming_graph_data=grafo) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[STEP1]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "subnodes=[] #nodes of the subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "subnodes+=categories[inputcat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=G.subgraph(nodes=subnodes) #creating the first subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "pesi={} #a dict with weights of each node\n",
    "ranking=[] #the ordered list of the ranking of each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank=[] \n",
    "# in_degree returns a dictionary with nodes as keys and in-degree as values\n",
    "# (The node in-degree is the number of edges pointing in to the node) \n",
    "for node in sub.in_degree: #for each node and starting weigth\n",
    "    nodo=node[0] \n",
    "    peso=node[1]\n",
    "    pesi[nodo]=peso #we set his weight in the dict\n",
    "    rank.append((peso, nodo)) #we append him to rank list\n",
    "for node in sorted(rank, reverse=True): #for each node, starting from the highest rank\n",
    "    ranking.append(node[1]) #we append the node to the final list of rankings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[STEP2&3]__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 34/34 [00:12<00:00,  3.15it/s]\n"
     ]
    }
   ],
   "source": [
    "for el in tqdm(blockranking[1:]): #for every category(except the input one)\n",
    "    rank=[] #we inialize the list of ranks of this category\n",
    "    cat=el[1]\n",
    "    catnodes=categories[cat]\n",
    "    subnodes+=catnodes\n",
    "    sub=G.subgraph(nodes=subnodes) #we add the nodes to the subgraph\n",
    "    for node in catnodes: #for each node of the category\n",
    "        peso=0 #the weigth is 0 at first \n",
    "        for pred in sub.predecessors(node): #predecessors return a list of predecessor nodes of n.\n",
    "            if pred in pesi: #if comes from an upper category\n",
    "                peso+=pesi[pred] #we add the weight in the dictionary\n",
    "            else: peso+=1\n",
    "        rank.append((peso, node)) #we append the node to the rank list \n",
    "    for node in sorted(rank, reverse=True): #for each node, starting from the highest rank\n",
    "        ranking.append(node[1]) #we append the node to the final list of rankings\n",
    "        pesi[node[1]]=node[0] #we update our dictionary of weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finished! Now let's take a look to the highest and lowest articles in our ranking list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diogenes Lartius\n",
      "Pausanias (geographer)\n",
      "Theocritus\n",
      "Dong Zhuo\n",
      "L Bu\n",
      "Yuan Shu\n",
      "Stobaeus\n",
      "Penda of Mercia\n",
      "Stephen Dingate\n",
      "Zhang Fei\n",
      "Sextus Empiricus\n",
      "Stigand\n",
      "Ivan Alexander of Bulgaria\n",
      "Tancred of Hauteville\n",
      "Pope Nicholas II\n",
      "Sweyn II of Denmark\n",
      "Amasis II\n",
      "Horemheb\n",
      "Nefertiti\n",
      "Pope Nicholas III\n"
     ]
    }
   ],
   "source": [
    "for node in ranking[0:20]:\n",
    "    print(articles[node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Viktor Dotsenko\n",
      "Ste Curran\n",
      "Wayne Smith (Chief Statistician of Canada)\n",
      "Natasha Alexandra\n",
      "Inayatullah (Guantanamo detainee 10029)\n",
      "Martin Hanlin\n",
      "Troy Dunn\n",
      "Margaret O'Leary\n",
      "David Getches\n",
      "Richard Orr\n",
      "Tony Medina\n",
      "Donald N. Sills\n",
      "Dave Glinka\n",
      "Bb Manga\n",
      "Roxane Berard\n",
      "Josateki Nawalowalo\n",
      "Charles W. Curtis\n",
      "Arnold J. Levine\n",
      "Alan Klingenstein\n",
      "Don Rondo\n"
     ]
    }
   ],
   "source": [
    "for node in ranking[-20:]:\n",
    "    print(articles[node])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![wiki](https://static1.squarespace.com/static/5a4fb547b0786969790f230b/t/5a626c17ec212d0fa48d9fe5/1516399656615/Wikipedia+Banner)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
